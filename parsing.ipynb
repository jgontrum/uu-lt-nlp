{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:17:29.923839Z",
     "start_time": "2017-12-10T18:17:29.913643Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global dependencies here\n",
    "from typing import Dict, List, Set, Iterable, Sequence, Tuple\n",
    "from collections import deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc-eager transition-based parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:43:52.705480Z",
     "start_time": "2017-12-12T07:43:50.480441Z"
    },
    "code_folding": [
     156,
     205
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionBasedParser(object):\n",
    "    \n",
    "    class Arc(object):\n",
    "        \n",
    "        def __init__(self, left_word: int, label: str, right_word: int,\n",
    "                     sentence: List[str]):\n",
    "            # sentence is required to start with \"ROOT\"\n",
    "            self.left = left_word\n",
    "            self.label = label\n",
    "            self.right = right_word\n",
    "            self.sentence = sentence\n",
    "            \n",
    "        def __repr__(self):\n",
    "            left = self.sentence[self.left]\n",
    "            right = self.sentence[self.right]\n",
    "            return f\"({left} --|{self.label}|--> {right})\"\n",
    "\n",
    "        \n",
    "    class Configuration(object):\n",
    "        \n",
    "        def __init__(self, stack: List[int], buffer: List[int],\n",
    "                     arcs: List, sentence: List[str], features=None):\n",
    "            self.stack = stack\n",
    "            self.buffer = deque(buffer)\n",
    "            self.arcs = arcs\n",
    "            self.sentence = sentence\n",
    "            self.features = features or []\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            stack = [self.sentence[i] for i in self.stack]\n",
    "            buffer = [self.sentence[i] for i in self.buffer]\n",
    "\n",
    "            return f\"<CONFIGURATION>\\n  stack={stack}\\n  buffer={buffer}\\n  arcs={self.arcs}\\n</CONFIGURATION>\"\n",
    "        \n",
    "        \n",
    "    class Graph(object):\n",
    "        \n",
    "        def __init__(self, arcs: List=None, features=None):\n",
    "            self.arcs = arcs or []\n",
    "            self.features = features or []\n",
    "\n",
    "        def starts_with(self, index):\n",
    "            return filter(lambda arc: arc.left == index, self.arcs)\n",
    "                \n",
    "        def ends_with(self, index):\n",
    "            return filter(lambda arc: arc.right == index, self.arcs)\n",
    "        \n",
    "        def contains_arc(self, left: int, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right, \n",
    "                           self.arcs)\n",
    "            \n",
    "        def contains_arc_label(self, left: int, label: str, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right\n",
    "                               and arc.label == label, \n",
    "                           self.arcs)\n",
    "        \n",
    "        def __repr__(self) -> str:\n",
    "            return str(self.arcs)\n",
    "        \n",
    "        def subset(self, other):\n",
    "            # Are all the arcs in the other graph in this one?\n",
    "            a = set([(a.left, a.label, a.right) for a in self.arcs])\n",
    "            b = set([(a.left, a.label, a.right) for a in other])\n",
    "\n",
    "            return b.difference(a)\n",
    "                    \n",
    "\n",
    "    \n",
    "    def __init__(self, labels: List[str], ml_data=None):\n",
    "        self.labels = labels\n",
    "        self.ml_data = ml_data\n",
    "    \n",
    "    def parse(self, sentence: List[str], features):\n",
    "        # Accepts a list of tokens\n",
    "        configuration = self._initialization(sentence)\n",
    "        configuration.features = features\n",
    "        print(\"Initial configuration:\")\n",
    "        print(configuration)\n",
    "\n",
    "        # Keep making transitions until the termination\n",
    "        # requirement is met.\n",
    "        while not self._check_termination(configuration):\n",
    "            print()\n",
    "            configuration = self._make_transition(configuration)\n",
    "            print(\"New configuration:\")\n",
    "            print(configuration)\n",
    "        \n",
    "        return configuration.arcs\n",
    "    \n",
    "    def _make_transition(self, c):\n",
    "        # Get features for the configuration\n",
    "        #     \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "\n",
    "        features = self.ml_data['feat'].featurize(c)\n",
    "        \n",
    "        # Now turn them into a vector\n",
    "        vector = self.ml_data['vec'].transform(features)\n",
    "        \n",
    "        prediction = self.ml_data['clf'].predict(vector)[0]\n",
    "        print(prediction)\n",
    "        if prediction == \"LA\":\n",
    "            print(\"Left arc\")\n",
    "            new_configuration = self._left_arc(self.labels[0], c)\n",
    "        elif prediction == \"RA\":\n",
    "            print(\"Right arc\")\n",
    "            new_configuration = self._right_arc(self.labels[0], c)\n",
    "        elif prediction == \"SH\":\n",
    "            print(\"Shift\")\n",
    "            new_configuration = self._shift(c)\n",
    "        else:\n",
    "            print(\"Reduce\")\n",
    "            new_configuration = self._reduce(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            print(\"Fallback Shift\")\n",
    "            new_configuration = self._shift(c)\n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _make_transition_naive(self, c):\n",
    "        # Two big questions:\n",
    "        # How to chose the correct label?\n",
    "        # How to chose the correct transition\n",
    "        label = self.labels[0]\n",
    "        new_configuration = None\n",
    "             \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._reduce(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._left_arc(label, c)\n",
    "\n",
    "        if not new_configuration:\n",
    "            new_configuration = self._shift(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._right_arc(label, c)  \n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _left_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "                \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Remove from stack\n",
    "        w_i = c.stack.pop()\n",
    "        \n",
    "        # Keep on buffer\n",
    "        w_j = c.buffer[0]\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_j -> w_i\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_j, label=label, right_word=w_i, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _right_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Keep on stack\n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Remove from buffer\n",
    "        w_j = c.buffer.popleft()\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_i -> w_j\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_i, label=label, right_word=w_j, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "        \n",
    "        # Append wj to stack\n",
    "        c.stack.append(w_j)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _reduce(self, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Seach for an arc leading to w_i\n",
    "        valid_arc = list(filter(lambda arc: arc.right == w_i, c.arcs))\n",
    "        \n",
    "        if not valid_arc:\n",
    "            return None\n",
    "        \n",
    "        # Remove w_i from the stack\n",
    "        c.stack.pop()\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def _shift(self, c):        \n",
    "        \n",
    "        c = deepcopy(c)\n",
    "        c.stack.append(c.buffer.popleft())\n",
    "        return c\n",
    "    \n",
    "    def _initialization(self, sentence: List[str]):\n",
    "        return TransitionBasedParser.Configuration(\n",
    "            stack=[0], buffer=list(range(1, len(sentence))),\n",
    "            arcs=[], sentence=sentence)\n",
    "    \n",
    "    def _check_termination(self, c) -> bool:\n",
    "        return not c or not c.buffer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:44.310212Z",
     "start_time": "2017-12-08T17:54:04.020598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Donload training dataset\n",
    "f = requests.get(\"https://www.dropbox.com/s/nnxvjk1n3wc5sof/ewt-train.conll?dl=1\")\n",
    "\n",
    "corpus = f.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus in the CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T09:10:01.132990Z",
     "start_time": "2017-12-10T09:10:01.104886Z"
    },
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from conllu.parser import parse\n",
    "\n",
    "class ConllTreeBankReader(object):\n",
    "        \n",
    "    @staticmethod\n",
    "    def parse(content: str):\n",
    "        # Iterates over text in CONLL format and generates\n",
    "        # Graphs with the arcs of the dependency tree\n",
    "        Arc = TransitionBasedParser.Arc\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "\n",
    "        for sent in parse(content):\n",
    "            \"\"\"\n",
    "            conllu output:\n",
    "            \n",
    "            OrderedDict([\n",
    "                ('id', 1),\n",
    "                ('form', 'The'),\n",
    "                ('lemma', 'the'),\n",
    "                ('upostag', 'DET'),\n",
    "                ('xpostag', 'DT'),\n",
    "                ('feats', OrderedDict([('Definite', 'Def'), ('PronType', 'Art')])),\n",
    "                ('head', 4),\n",
    "                ('deprel', 'det'),\n",
    "                ('deps', None),\n",
    "                ('misc', None)\n",
    "            ])\n",
    "            \"\"\"\n",
    "            \n",
    "            sentence_str = [\"ROOT\"] + [t['form'] for t in sent]\n",
    "            \n",
    "            yield Graph([Arc(\n",
    "                left_word=t['head'],\n",
    "                label=t['deprel'],\n",
    "                right_word=t['id'],\n",
    "                sentence=sentence_str\n",
    "            ) for t in sent],\n",
    "            features=sent)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a dependency graph into a sequence of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T09:18:05.567713Z",
     "start_time": "2017-12-10T09:18:05.274325Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConfigurationExtractor(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_configurations(graphs: Sequence[TransitionBasedParser.Graph]):\n",
    "        Config = TransitionBasedParser.Configuration\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "        parser = TransitionBasedParser(None)\n",
    "        \n",
    "        for graph in graphs:\n",
    "            # Skip sentences without arcs\n",
    "            if not graph.arcs:\n",
    "                continue\n",
    "                                \n",
    "            sentence = graph.arcs[0].sentence\n",
    "            features = graph.features\n",
    "            \n",
    "            \"\"\" Generate initial configuration:\n",
    "            \n",
    "            For every instance (S_d , G_d) ∈ D, we ﬁrst construct a transition sequence \n",
    "            C_(0,m) = (c_0, c_1, ... , c_m) such that\n",
    "\n",
    "                1. c_0 = c_0(S_d), <====\n",
    "                2. G_d = (V_d, A_c_m).\n",
    "            \"\"\"\n",
    "            \n",
    "            configuration = parser._initialization(sentence)\n",
    "            configuration.features = features\n",
    "            yield \"INIT\", None, configuration\n",
    "            \n",
    "            \"\"\" Use the oracle function to generate all\n",
    "            non-terminal configurations:\n",
    "        \n",
    "            See: Algorithm 1 in:\n",
    "            \"A Dynamic Oracle for Arc-Eager Dependency Parsing\"\n",
    "            \"\"\"\n",
    "            \n",
    "            i = 500\n",
    "            while not parser._check_termination(configuration):\n",
    "                # assert not graph.subset(configuration.arcs)\n",
    "                \n",
    "                # Make sure not to end up in an infinite loop\n",
    "                i -= 1\n",
    "                if i <= 0:\n",
    "                    break\n",
    "                    \n",
    "                # For all arcs, we need the buffer and stack to be not empty\n",
    "                if len(configuration.stack) > 0 and len(configuration.buffer) > 0:\n",
    "                    buffer_0 = configuration.buffer[0]\n",
    "                    stack_0 = configuration.stack[-1]\n",
    "\n",
    "                    # Check for left-arc:\n",
    "                    # if (β[0], r, σ[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(buffer_0, stack_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._left_arc(label, configuration)\n",
    "                        configuration.features = features\n",
    "                        yield \"LA\", label, configuration\n",
    "                        continue\n",
    "\n",
    "                    # Check for right-arc:\n",
    "                    # if (σ[0], r, β[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(stack_0, buffer_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._right_arc(label, configuration)\n",
    "                        configuration.features = features\n",
    "                        yield \"RA\", label, configuration\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for reduce\n",
    "                    success = False\n",
    "                    for k in range(stack_0):\n",
    "                        arcs = list(graph.contains_arc(k, buffer_0))\n",
    "                        arcs += list(graph.contains_arc(buffer_0, k))\n",
    "                        if arcs:\n",
    "                            if parser._reduce(configuration) is None:\n",
    "                                break\n",
    "                            \n",
    "                            configuration = parser._reduce(configuration)\n",
    "                            configuration.features = features\n",
    "                            yield \"RE\", None, configuration\n",
    "                            success = True\n",
    "                            break\n",
    "                    if success:\n",
    "                        continue\n",
    "                            \n",
    "                # Else: Shift\n",
    "                configuration = parser._shift(configuration)\n",
    "                configuration.features = features\n",
    "                yield \"SH\", None, configuration\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a configuration into a FeatureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:32:12.798293Z",
     "start_time": "2017-12-10T18:32:12.610571Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_dep_on_buffer\": \"punct\",\n",
      "  \"first_dep_on_stack\": \"root\",\n",
      "  \"first_lemma_on_buffer\": \"-\",\n",
      "  \"first_lemma_on_stack\": \"Al\",\n",
      "  \"first_on_buffer_incoming\": \"-\",\n",
      "  \"first_on_buffer_outgoing\": \"-\",\n",
      "  \"first_on_stack_incoming\": \"ROOT\",\n",
      "  \"first_on_stack_outgoing\": \"-\",\n",
      "  \"first_pos_on_buffer\": \"PUNCT\",\n",
      "  \"first_pos_on_stack\": \"PROPN\",\n",
      "  \"second_dep_on_buffer\": \"flat\",\n",
      "  \"second_dep_on_stack\": \"-\",\n",
      "  \"second_lemma_on_buffer\": \"Zaman\",\n",
      "  \"second_lemma_on_stack\": \"ROOT\",\n",
      "  \"second_pos_on_buffer\": \"PROPN\",\n",
      "  \"second_pos_on_stack\": \"ROOT\"\n",
      "}\n",
      "{\n",
      "  \"first_dep_on_buffer\": \"flat\",\n",
      "  \"first_dep_on_stack\": \"punct\",\n",
      "  \"first_lemma_on_buffer\": \"Zaman\",\n",
      "  \"first_lemma_on_stack\": \"-\",\n",
      "  \"first_on_buffer_incoming\": \"PROPN\",\n",
      "  \"first_on_buffer_outgoing\": \"-\",\n",
      "  \"first_on_stack_incoming\": \"-\",\n",
      "  \"first_on_stack_outgoing\": \"PUNCT\",\n",
      "  \"first_pos_on_buffer\": \"PROPN\",\n",
      "  \"first_pos_on_stack\": \"PUNCT\",\n",
      "  \"second_dep_on_buffer\": \"punct\",\n",
      "  \"second_dep_on_stack\": \"root\",\n",
      "  \"second_lemma_on_buffer\": \":\",\n",
      "  \"second_lemma_on_stack\": \"Al\",\n",
      "  \"second_pos_on_buffer\": \"PUNCT\",\n",
      "  \"second_pos_on_stack\": \"PROPN\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Featurizer(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_from_list(l, pos, sent):\n",
    "        if (pos >= 0 and pos < len(l)) or (pos < 0 and len(l) + pos >= 0):\n",
    "            return sent[l[pos]]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @staticmethod\n",
    "    def featurize(c):\n",
    "        s = c.sentence\n",
    "        f = c.features\n",
    "        a = c.arcs\n",
    "        g = TransitionBasedParser.Graph(a)\n",
    "        \n",
    "        lemmas = [\"ROOT\"] + [x['lemma'] for x in f]\n",
    "        pos = [\"ROOT\"] + [x['upostag'] for x in f]\n",
    "        deps = [\"-\"] + [x['deprel'] for x in f]\n",
    "        \n",
    "        outgoing = []\n",
    "        for x in range(len(s)):\n",
    "            for arc in g.starts_with(x):\n",
    "                outgoing.append(arc.right)\n",
    "                continue\n",
    "            outgoing.append(None)\n",
    "        \n",
    "        incoming = []\n",
    "        for x in range(len(s)):\n",
    "            for arc in g.ends_with(x):\n",
    "                incoming.append(arc.left)\n",
    "                continue\n",
    "            incoming.append(None)\n",
    "\n",
    "        return {\n",
    "#             \"first_word_on_stack\": Featurizer.get_from_list(c.stack, -1, s),\n",
    "            \"first_lemma_on_stack\": Featurizer.get_from_list(c.stack, -1, lemmas),\n",
    "            \"first_pos_on_stack\": Featurizer.get_from_list(c.stack, -1, pos),\n",
    "            \"first_dep_on_stack\": Featurizer.get_from_list(c.stack, -1, deps),\n",
    "            \"first_on_stack_incoming\": Featurizer.get_from_list(incoming, c.stack[-1], pos) \n",
    "                if c.stack and incoming[c.stack[-1]] is not None else \"-\",\n",
    "            \"first_on_stack_outgoing\": Featurizer.get_from_list(outgoing, c.stack[-1], pos) \n",
    "                if c.stack and outgoing[c.stack[-1]]is not None else \"-\",\n",
    "            \n",
    "#             \"second_word_on_stack\": Featurizer.get_from_list(c.stack, -2, s),\n",
    "            \"second_lemma_on_stack\": Featurizer.get_from_list(c.stack, -2, lemmas),\n",
    "            \"second_pos_on_stack\": Featurizer.get_from_list(c.stack, -2, pos),\n",
    "            \"second_dep_on_stack\": Featurizer.get_from_list(c.stack, -2, deps),\n",
    "\n",
    "#             \"first_word_on_buffer\": Featurizer.get_from_list(c.buffer, 0, s),\n",
    "            \"first_lemma_on_buffer\": Featurizer.get_from_list(c.buffer, 0, lemmas),\n",
    "            \"first_pos_on_buffer\": Featurizer.get_from_list(c.buffer, 0, pos),\n",
    "            \"first_dep_on_buffer\": Featurizer.get_from_list(c.buffer, 0, deps),\n",
    "             \"first_on_buffer_incoming\": Featurizer.get_from_list(incoming, c.buffer[0], pos) \n",
    "                if c.buffer and incoming[c.buffer[0]] is not None else \"-\",\n",
    "            \"first_on_buffer_outgoing\": Featurizer.get_from_list(outgoing, c.buffer[0], pos) \n",
    "                if c.buffer and outgoing[c.buffer[0]]is not None else \"-\",\n",
    "            \n",
    "#             \"second_word_on_buffer\": Featurizer.get_from_list(c.buffer, 1, s),\n",
    "            \"second_lemma_on_buffer\": Featurizer.get_from_list(c.buffer, 1, lemmas),\n",
    "            \"second_pos_on_buffer\": Featurizer.get_from_list(c.buffer, 1, pos),\n",
    "            \"second_dep_on_buffer\": Featurizer.get_from_list(c.buffer, 1, deps),\n",
    "        }\n",
    "           \n",
    "# for i, x in enumerate(configs):\n",
    "#     if i > 1:\n",
    "#         break\n",
    "#     print(json.dumps(Featurizer.featurize(x), indent=2, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus and turn the configurations into feature dicts. \n",
    "\n",
    "This can take up to 10-15 minutes, depending on the size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:32:12.607748Z",
     "start_time": "2017-12-10T18:20:41.908245Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "y = []\n",
    "\n",
    "for i, (transition, label, config) in enumerate(\n",
    "        ConfigurationExtractor.generate_configurations(\n",
    "            ConllTreeBankReader.parse(corpus))):\n",
    "    if not config or transition == \"INIT\":\n",
    "        continue\n",
    "        \n",
    "    configs.append(config)\n",
    "    \n",
    "    if not label:\n",
    "        label = \"\"\n",
    "        \n",
    "    y.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:36:08.277700Z",
     "start_time": "2017-12-10T18:34:11.797676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dicts = [Featurizer.featurize(x) for x in configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create the actual numpy feature vectors using a dict vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:37:48.826258Z",
     "start_time": "2017-12-10T18:37:35.162637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Todo: Shuffle data\n",
    "\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-valdation to find the best classifier\n",
    "\n",
    "### Warning: This might take hours :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T21:23:59.079104Z",
     "start_time": "2017-12-10T18:41:09.695356Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "Avg. accuracy: 0.2496 (+/- 0.0011)\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.8008 (+/- 0.0045)\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Avg. accuracy: 0.7896 (+/- 0.0056)\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Avg. accuracy: 0.7155 (+/- 0.0060)\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.8207 (+/- 0.0052)\n",
      "\n",
      " Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=100, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.7415 (+/- 0.0071)\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
       "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers_to_test = [\n",
    "    DummyClassifier(strategy='uniform'),\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    BernoulliNB(),\n",
    "    RandomForestClassifier(),\n",
    "    Perceptron(n_iter=100),\n",
    "#     KNeighborsClassifier(weights=\"distance\"), # <- theres sth wrong with it Oo\n",
    "]\n",
    "\n",
    "from sklearn import cross_validation\n",
    "for clf in classifiers_to_test:\n",
    "    print(\"\\n\", clf)\n",
    "    # Train 10 times, each time holding 10% of the data out to validate on.\n",
    "    # The overall accuracy is then an average of the 10 training results.\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    print(\"Avg. accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results (accuracy)\n",
    "\n",
    "| Run | Baseline | LogisticRegression | SVC | Bernoulli Naive Bayes | Random Forest  | Perceptron | Nearest Neighbor |\n",
    "|-----|----------|--------------------|-----|-----------------------|----------------|------------|------------------|\n",
    "|  1  | 0.2496  | 0.8008 | 0.7896 | 0.7155 | **0.8207** |  0.7415 | | |\n",
    "\n",
    "\n",
    "Run 1: Features (example): \n",
    "```\n",
    "[\"first_dep_on_buffer\", \"first_dep_on_stack\", \"first_lemma_on_buffer\", \"first_lemma_on_stack\", \"first_on_buffer_incoming\", \"first_on_buffer_outgoing\", \"first_on_stack_incoming\", \"first_on_stack_outgoing\",  \"first_pos_on_buffer\", \"first_pos_on_stack\", \"second_dep_on_buffer\", \"second_dep_on_stack\", \"second_lemma_on_buffer\",  \"second_lemma_on_stack\", \"second_pos_on_buffer\", \"second_pos_on_stack\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T21:28:03.701365Z",
     "start_time": "2017-12-10T21:24:16.285416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(n_jobs=-1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FINAL PARSER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:44:19.215918Z",
     "start_time": "2017-12-12T07:44:19.206129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = v\n",
    "featurizer = Featurizer\n",
    "classifier = clf\n",
    "\n",
    "parser = TransitionBasedParser([\"label\"], ml_data={\n",
    "    \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:44:45.961184Z",
     "start_time": "2017-12-12T07:44:45.898883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[OrderedDict([('id', 1), ('form', 'He'), ('lemma', 'he'), ('upostag', 'PRON'), ('xpostag', 'PRON'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 2), ('form', 'worked'), ('lemma', 'work'), ('upostag', 'VERB'), ('xpostag', 'VERB'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 3), ('form', 'for'), ('lemma', 'for'), ('upostag', 'ADP'), ('xpostag', 'ADP'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 4), ('form', 'the'), ('lemma', 'the'), ('upostag', 'DET'), ('xpostag', 'DET'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 5), ('form', 'BBC'), ('lemma', 'BBC'), ('upostag', 'PROPN'), ('xpostag', 'PROPN'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 6), ('form', 'for'), ('lemma', 'for'), ('upostag', 'ADP'), ('xpostag', 'ADP'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 7), ('form', 'a'), ('lemma', 'a'), ('upostag', 'DET'), ('xpostag', 'DET'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 8), ('form', 'decade'), ('lemma', 'decade'), ('upostag', 'NOUN'), ('xpostag', 'NOUN'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 9), ('form', '.'), ('lemma', '.'), ('upostag', 'PUNCT'), ('xpostag', 'PUNCT'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)])]]\n",
      "Initial configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['ROOT']\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['He']\n",
      "  buffer=['worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['worked']\n",
      "  buffer=['for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['for']\n",
      "  buffer=['the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['the']\n",
      "  buffer=['BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC']\n",
      "  buffer=['for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for']\n",
      "  buffer=['a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for', 'a']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['decade']\n",
      "  buffer=['.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC), (. --|label|--> decade)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['.']\n",
      "  buffer=[]\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC), (. --|label|--> decade)]\n",
      "</CONFIGURATION>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(He --|label|--> ROOT),\n",
       " (worked --|label|--> He),\n",
       " (for --|label|--> worked),\n",
       " (the --|label|--> for),\n",
       " (BBC --|label|--> the),\n",
       " (BBC --|label|--> for),\n",
       " (for --|label|--> a),\n",
       " (decade --|label|--> a),\n",
       " (decade --|label|--> for),\n",
       " (decade --|label|--> BBC),\n",
       " (. --|label|--> decade)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from conllu.parser import parse\n",
    "\n",
    "sent = \"\"\"\n",
    "1\tHe\the\tPRON\tPRON\t_\t_\t_\t_\t_\n",
    "2\tworked\twork\tVERB\tVERB\t_\t_\t_\t_\t_\n",
    "3\tfor\tfor\tADP\tADP\t_\t_\t_\t_\t_\n",
    "4\tthe\tthe\tDET\tDET\t_\t_\t_\t_\t_\n",
    "5\tBBC\tBBC\tPROPN\tPROPN\t_\t_\t_\t_\t_\n",
    "6\tfor\tfor\tADP\tADP\t_\t_\t_\t_\t_\n",
    "7\ta\ta\tDET\tDET\t_\t_\t_\t_\t_\n",
    "8\tdecade\tdecade\tNOUN\tNOUN\t_\t_\t_\t_\t_\n",
    "9\t.\t.\tPUNCT\tPUNCT\t_\t_\t_\t_\t_\n",
    "\"\"\"\n",
    "\n",
    "features_test = parse(sent)\n",
    "\n",
    "print(features_test)\n",
    "\n",
    "parser.parse([\"ROOT\"] + [x['form'] for x in features_test[0]], features_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Constituency Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T18:16:23.963525Z",
     "start_time": "2017-12-08T18:15:06.285Z"
    },
    "code_folding": [
     0,
     13,
     57
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rule(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                lhs: str,\n",
    "                rhs: List[str],\n",
    "                prob: float=1.0):\n",
    "        self.lhs = lhs\n",
    "        self.rhs = rhs\n",
    "        self.prob = prob\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"'{self.lhs}' -> {self.rhs} [{self.prob}]\"\n",
    "\n",
    "class Grammar(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 start: str,\n",
    "                 rules: Iterable[Rule],\n",
    "                 non_terminals: Iterable[str]=None,\n",
    "                 terminals: Iterable[str]=None):\n",
    "        # Leave nonterminals and terminals empty to infer them from the rules\n",
    "        self.start = start\n",
    "        self.rules = set(rules)\n",
    "        \n",
    "        if non_terminals:\n",
    "            self.non_terminals = set(non_terminals)\n",
    "        if terminals:\n",
    "            self.terminals = set(terminals)\n",
    "            \n",
    "        if non_terminals is None or terminals is None:\n",
    "            self.non_terminals = set()\n",
    "            self.terminals = set()\n",
    "            \n",
    "            for rule in self.rules:\n",
    "                self.non_terminals.add(rule.lhs)\n",
    "                if len(rule.rhs) == 1:\n",
    "                    self.terminals.add(rule.rhs[0])\n",
    "                elif len(rule.rhs) == 2:\n",
    "                    self.non_terminals.add(rule.rhs[0])\n",
    "                    self.non_terminals.add(rule.rhs[1])\n",
    "                else:\n",
    "                    raise ValueError(\"Only rules in Chomsky Normal Form supported.\")\n",
    "        \n",
    "        self.non_terminals.add(start)\n",
    "        \n",
    "    def get_lhs_for_rhs(self, rhs: Sequence[str]) -> Set[str]:\n",
    "        rhs = list(rhs)\n",
    "        x = set(rule.lhs for rule in\n",
    "            filter(lambda rule: rule.rhs == rhs, self.rules)\n",
    "        )\n",
    "        if x:\n",
    "            print(list(filter(lambda rule: rule.rhs == rhs, self.rules)))\n",
    "        return x\n",
    "                \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"({self.start}, {self.non_terminals}, {self.terminals}, {self.rules})\"\n",
    "\n",
    "class CYKParser(object):\n",
    "    \n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "        \n",
    "    def draw_table(self, table: List[List[Set[str]]]) -> str:\n",
    "        ret = []\n",
    "        \n",
    "        for i, row in enumerate(table):\n",
    "            row_ret = \"\"\n",
    "            for j, cell in enumerate(row):\n",
    "                row_ret += f\"[{i},{j}: {cell}]\"\n",
    "            ret.append(row_ret)\n",
    "        \n",
    "        return \"\\n\".join(ret)\n",
    "        \n",
    "    def recognize(self, sentence: Sequence[str]) -> bool:\n",
    "        # Implemented like Figure 13.10 in J&M p.474\n",
    "        \n",
    "        sentence = list(sentence)\n",
    "        \n",
    "        # Fill the table, a list of lists.\n",
    "        table = [\n",
    "            [set() for _ in range(len(sentence))]\n",
    "            for _ in range(len(sentence))\n",
    "        ]\n",
    "                \n",
    "        # Iterate word by word. \n",
    "        # Note: The indices may be off by one compared to the \n",
    "        # pseudo code in the book due to pythons lists etc.\n",
    "        for j in range(len(sentence)):\n",
    "            # Put the POS tag for the current terminal in the diagonal cells\n",
    "            print()\n",
    "            table[j][j] = self.grammar.get_lhs_for_rhs([sentence[j]])\n",
    "\n",
    "            for i in range(j, -1, -1):\n",
    "                for k in range(i, j):\n",
    "                    # NT1 describes the span from i to k,\n",
    "                    # NT2 the one from k to j\n",
    "                    # If we find something that goes X -> NT1 NT2,\n",
    "                    # we can conver the span i to j!\n",
    "                    print(f\"j={j} | i={i}\")\n",
    "\n",
    "                    non_terminals_1 = table[i][k]\n",
    "                    non_terminals_2 = table[k + 1][j]\n",
    "                    \n",
    "                    \n",
    "                    for rhs in itertools.product(non_terminals_1, non_terminals_2):\n",
    "                        # Find a left hand side from rules that produce\n",
    "                        # both non-terminals. Add them (set: union) to the table.\n",
    "                        table[i][j] = table[i][j] | self.grammar.get_lhs_for_rhs(rhs)\n",
    "\n",
    "            \n",
    "        print(self.draw_table(table))\n",
    "        return self.grammar.start in table[0][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T18:16:24.015343Z",
     "start_time": "2017-12-08T18:15:06.755Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules = {\n",
    "    Rule(\"SS\", \"S Punct\".split()),\n",
    "    Rule(\"S\", \"NP VP\".split()),\n",
    "    Rule(\"S\", \"Pron VP\".split()),\n",
    "    Rule(\"NP\", \"Det Noun\".split()),\n",
    "    Rule(\"VP\", \"Verb VP1\".split()),\n",
    "    Rule(\"VP1\", \"PP PP\".split()),\n",
    "    Rule(\"PP\", \"Prep NP\".split()),\n",
    "    Rule(\"PP\", \"Prep Pron\".split()),\n",
    "    Rule(\"Noun\", [\"BBC\"]),\n",
    "    Rule(\"Noun\", [\"decade\"]),\n",
    "    Rule(\"Verb\", [\"worked\"]),\n",
    "    Rule(\"Pron\", [\"He\"]),\n",
    "    Rule(\"Det\", [\"the\"]),\n",
    "    Rule(\"Det\", [\"a\"]),\n",
    "    Rule(\"Prep\", [\"for\"]),\n",
    "    Rule(\"Punct\", [\".\"])\n",
    "}\n",
    "\n",
    "g = Grammar(\"SS\", rules)\n",
    "\n",
    "p = CYKParser(g)\n",
    "\n",
    "print(p.recognize(\"He worked for the BBC for a decade .\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
