{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:17:29.923839Z",
     "start_time": "2017-12-10T18:17:29.913643Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global dependencies here\n",
    "from typing import Dict, List, Set, Iterable, Sequence, Tuple\n",
    "from collections import deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc-eager transition-based parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:43:52.705480Z",
     "start_time": "2017-12-12T07:43:50.480441Z"
    },
    "code_folding": [
     156,
     205
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionBasedParser(object):\n",
    "    \n",
    "    class Arc(object):\n",
    "        \n",
    "        def __init__(self, left_word: int, label: str, right_word: int,\n",
    "                     sentence: List[str]):\n",
    "            # sentence is required to start with \"ROOT\"\n",
    "            self.left = left_word\n",
    "            self.label = label\n",
    "            self.right = right_word\n",
    "            self.sentence = sentence\n",
    "            \n",
    "        def __repr__(self):\n",
    "            left = self.sentence[self.left]\n",
    "            right = self.sentence[self.right]\n",
    "            return f\"({left} --|{self.label}|--> {right})\"\n",
    "\n",
    "        \n",
    "    class Configuration(object):\n",
    "        \n",
    "        def __init__(self, stack: List[int], buffer: List[int],\n",
    "                     arcs: List, sentence: List[str], features=None):\n",
    "            self.stack = stack\n",
    "            self.buffer = deque(buffer)\n",
    "            self.arcs = arcs\n",
    "            self.sentence = sentence\n",
    "            self.features = features or []\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            stack = [self.sentence[i] for i in self.stack]\n",
    "            buffer = [self.sentence[i] for i in self.buffer]\n",
    "\n",
    "            return f\"<CONFIGURATION>\\n  stack={stack}\\n  buffer={buffer}\\n  arcs={self.arcs}\\n</CONFIGURATION>\"\n",
    "        \n",
    "        \n",
    "    class Graph(object):\n",
    "        \n",
    "        def __init__(self, arcs: List=None, features=None):\n",
    "            self.arcs = arcs or []\n",
    "            self.features = features or []\n",
    "\n",
    "        def starts_with(self, index):\n",
    "            return filter(lambda arc: arc.left == index, self.arcs)\n",
    "                \n",
    "        def ends_with(self, index):\n",
    "            return filter(lambda arc: arc.right == index, self.arcs)\n",
    "        \n",
    "        def contains_arc(self, left: int, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right, \n",
    "                           self.arcs)\n",
    "            \n",
    "        def contains_arc_label(self, left: int, label: str, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right\n",
    "                               and arc.label == label, \n",
    "                           self.arcs)\n",
    "        \n",
    "        def __repr__(self) -> str:\n",
    "            return str(self.arcs)\n",
    "        \n",
    "        def subset(self, other):\n",
    "            # Are all the arcs in the other graph in this one?\n",
    "            a = set([(a.left, a.label, a.right) for a in self.arcs])\n",
    "            b = set([(a.left, a.label, a.right) for a in other])\n",
    "\n",
    "            return b.difference(a)\n",
    "                    \n",
    "\n",
    "    \n",
    "    def __init__(self, labels: List[str], ml_data=None):\n",
    "        self.labels = labels\n",
    "        self.ml_data = ml_data\n",
    "    \n",
    "    def parse(self, sentence: List[str], features):\n",
    "        # Accepts a list of tokens\n",
    "        configuration = self._initialization(sentence)\n",
    "        configuration.features = features\n",
    "        print(\"Initial configuration:\")\n",
    "        print(configuration)\n",
    "\n",
    "        # Keep making transitions until the termination\n",
    "        # requirement is met.\n",
    "        while not self._check_termination(configuration):\n",
    "            print()\n",
    "            configuration = self._make_transition(configuration)\n",
    "            print(\"New configuration:\")\n",
    "            print(configuration)\n",
    "        \n",
    "        return configuration.arcs\n",
    "    \n",
    "    def _make_transition(self, c):\n",
    "        # Get features for the configuration\n",
    "        #     \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "\n",
    "        features = self.ml_data['feat'].featurize(c)\n",
    "        \n",
    "        # Now turn them into a vector\n",
    "        vector = self.ml_data['vec'].transform(features)\n",
    "        \n",
    "        prediction = self.ml_data['clf'].predict(vector)[0]\n",
    "        print(prediction)\n",
    "        if prediction == \"LA\":\n",
    "            print(\"Left arc\")\n",
    "            new_configuration = self._left_arc(self.labels[0], c)\n",
    "        elif prediction == \"RA\":\n",
    "            print(\"Right arc\")\n",
    "            new_configuration = self._right_arc(self.labels[0], c)\n",
    "        elif prediction == \"SH\":\n",
    "            print(\"Shift\")\n",
    "            new_configuration = self._shift(c)\n",
    "        else:\n",
    "            print(\"Reduce\")\n",
    "            new_configuration = self._reduce(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            print(\"Fallback Shift\")\n",
    "            new_configuration = self._shift(c)\n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _make_transition_naive(self, c):\n",
    "        # Two big questions:\n",
    "        # How to chose the correct label?\n",
    "        # How to chose the correct transition\n",
    "        label = self.labels[0]\n",
    "        new_configuration = None\n",
    "             \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._reduce(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._left_arc(label, c)\n",
    "\n",
    "        if not new_configuration:\n",
    "            new_configuration = self._shift(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._right_arc(label, c)  \n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _left_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "                \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Remove from stack\n",
    "        w_i = c.stack.pop()\n",
    "        \n",
    "        # Keep on buffer\n",
    "        w_j = c.buffer[0]\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_j -> w_i\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_j, label=label, right_word=w_i, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _right_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Keep on stack\n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Remove from buffer\n",
    "        w_j = c.buffer.popleft()\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_i -> w_j\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_i, label=label, right_word=w_j, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "        \n",
    "        # Append wj to stack\n",
    "        c.stack.append(w_j)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _reduce(self, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Seach for an arc leading to w_i\n",
    "        valid_arc = list(filter(lambda arc: arc.right == w_i, c.arcs))\n",
    "        \n",
    "        if not valid_arc:\n",
    "            return None\n",
    "        \n",
    "        # Remove w_i from the stack\n",
    "        c.stack.pop()\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def _shift(self, c):        \n",
    "        \n",
    "        c = deepcopy(c)\n",
    "        c.stack.append(c.buffer.popleft())\n",
    "        return c\n",
    "    \n",
    "    def _initialization(self, sentence: List[str]):\n",
    "        return TransitionBasedParser.Configuration(\n",
    "            stack=[0], buffer=list(range(1, len(sentence))),\n",
    "            arcs=[], sentence=sentence)\n",
    "    \n",
    "    def _check_termination(self, c) -> bool:\n",
    "        return not c or not c.buffer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:44.310212Z",
     "start_time": "2017-12-08T17:54:04.020598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Donload training dataset\n",
    "f = requests.get(\"https://www.dropbox.com/s/nnxvjk1n3wc5sof/ewt-train.conll?dl=1\")\n",
    "\n",
    "corpus = f.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus in the CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T09:10:01.132990Z",
     "start_time": "2017-12-10T09:10:01.104886Z"
    },
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from conllu.parser import parse\n",
    "\n",
    "class ConllTreeBankReader(object):\n",
    "        \n",
    "    @staticmethod\n",
    "    def parse(content: str):\n",
    "        # Iterates over text in CONLL format and generates\n",
    "        # Graphs with the arcs of the dependency tree\n",
    "        Arc = TransitionBasedParser.Arc\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "\n",
    "        for sent in parse(content):\n",
    "            \"\"\"\n",
    "            conllu output:\n",
    "            \n",
    "            OrderedDict([\n",
    "                ('id', 1),\n",
    "                ('form', 'The'),\n",
    "                ('lemma', 'the'),\n",
    "                ('upostag', 'DET'),\n",
    "                ('xpostag', 'DT'),\n",
    "                ('feats', OrderedDict([('Definite', 'Def'), ('PronType', 'Art')])),\n",
    "                ('head', 4),\n",
    "                ('deprel', 'det'),\n",
    "                ('deps', None),\n",
    "                ('misc', None)\n",
    "            ])\n",
    "            \"\"\"\n",
    "            \n",
    "            sentence_str = [\"ROOT\"] + [t['form'] for t in sent]\n",
    "            \n",
    "            yield Graph([Arc(\n",
    "                left_word=t['head'],\n",
    "                label=t['deprel'],\n",
    "                right_word=t['id'],\n",
    "                sentence=sentence_str\n",
    "            ) for t in sent],\n",
    "            features=sent)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a dependency graph into a sequence of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T09:18:05.567713Z",
     "start_time": "2017-12-10T09:18:05.274325Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConfigurationExtractor(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_configurations(graphs: Sequence[TransitionBasedParser.Graph]):\n",
    "        Config = TransitionBasedParser.Configuration\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "        parser = TransitionBasedParser(None)\n",
    "        \n",
    "        for graph in graphs:\n",
    "            # Skip sentences without arcs\n",
    "            if not graph.arcs:\n",
    "                continue\n",
    "                                \n",
    "            sentence = graph.arcs[0].sentence\n",
    "            features = graph.features\n",
    "            \n",
    "            \"\"\" Generate initial configuration:\n",
    "            \n",
    "            For every instance (S_d , G_d) ∈ D, we ﬁrst construct a transition sequence \n",
    "            C_(0,m) = (c_0, c_1, ... , c_m) such that\n",
    "\n",
    "                1. c_0 = c_0(S_d), <====\n",
    "                2. G_d = (V_d, A_c_m).\n",
    "            \"\"\"\n",
    "            \n",
    "            configuration = parser._initialization(sentence)\n",
    "            configuration.features = features\n",
    "            yield \"INIT\", None, configuration\n",
    "            \n",
    "            \"\"\" Use the oracle function to generate all\n",
    "            non-terminal configurations:\n",
    "        \n",
    "            See: Algorithm 1 in:\n",
    "            \"A Dynamic Oracle for Arc-Eager Dependency Parsing\"\n",
    "            \"\"\"\n",
    "            \n",
    "            i = 500\n",
    "            while not parser._check_termination(configuration):\n",
    "                # assert not graph.subset(configuration.arcs)\n",
    "                \n",
    "                # Make sure not to end up in an infinite loop\n",
    "                i -= 1\n",
    "                if i <= 0:\n",
    "                    break\n",
    "                    \n",
    "                # For all arcs, we need the buffer and stack to be not empty\n",
    "                if len(configuration.stack) > 0 and len(configuration.buffer) > 0:\n",
    "                    buffer_0 = configuration.buffer[0]\n",
    "                    stack_0 = configuration.stack[-1]\n",
    "\n",
    "                    # Check for left-arc:\n",
    "                    # if (β[0], r, σ[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(buffer_0, stack_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._left_arc(label, configuration)\n",
    "                        configuration.features = features\n",
    "                        yield \"LA\", label, configuration\n",
    "                        continue\n",
    "\n",
    "                    # Check for right-arc:\n",
    "                    # if (σ[0], r, β[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(stack_0, buffer_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._right_arc(label, configuration)\n",
    "                        configuration.features = features\n",
    "                        yield \"RA\", label, configuration\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for reduce\n",
    "                    success = False\n",
    "                    for k in range(stack_0):\n",
    "                        arcs = list(graph.contains_arc(k, buffer_0))\n",
    "                        arcs += list(graph.contains_arc(buffer_0, k))\n",
    "                        if arcs:\n",
    "                            if parser._reduce(configuration) is None:\n",
    "                                break\n",
    "                            \n",
    "                            configuration = parser._reduce(configuration)\n",
    "                            configuration.features = features\n",
    "                            yield \"RE\", None, configuration\n",
    "                            success = True\n",
    "                            break\n",
    "                    if success:\n",
    "                        continue\n",
    "                            \n",
    "                # Else: Shift\n",
    "                configuration = parser._shift(configuration)\n",
    "                configuration.features = features\n",
    "                yield \"SH\", None, configuration\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a configuration into a FeatureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:32:12.798293Z",
     "start_time": "2017-12-10T18:32:12.610571Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_dep_on_buffer\": \"punct\",\n",
      "  \"first_dep_on_stack\": \"root\",\n",
      "  \"first_lemma_on_buffer\": \"-\",\n",
      "  \"first_lemma_on_stack\": \"Al\",\n",
      "  \"first_on_buffer_incoming\": \"-\",\n",
      "  \"first_on_buffer_outgoing\": \"-\",\n",
      "  \"first_on_stack_incoming\": \"ROOT\",\n",
      "  \"first_on_stack_outgoing\": \"-\",\n",
      "  \"first_pos_on_buffer\": \"PUNCT\",\n",
      "  \"first_pos_on_stack\": \"PROPN\",\n",
      "  \"second_dep_on_buffer\": \"flat\",\n",
      "  \"second_dep_on_stack\": \"-\",\n",
      "  \"second_lemma_on_buffer\": \"Zaman\",\n",
      "  \"second_lemma_on_stack\": \"ROOT\",\n",
      "  \"second_pos_on_buffer\": \"PROPN\",\n",
      "  \"second_pos_on_stack\": \"ROOT\"\n",
      "}\n",
      "{\n",
      "  \"first_dep_on_buffer\": \"flat\",\n",
      "  \"first_dep_on_stack\": \"punct\",\n",
      "  \"first_lemma_on_buffer\": \"Zaman\",\n",
      "  \"first_lemma_on_stack\": \"-\",\n",
      "  \"first_on_buffer_incoming\": \"PROPN\",\n",
      "  \"first_on_buffer_outgoing\": \"-\",\n",
      "  \"first_on_stack_incoming\": \"-\",\n",
      "  \"first_on_stack_outgoing\": \"PUNCT\",\n",
      "  \"first_pos_on_buffer\": \"PROPN\",\n",
      "  \"first_pos_on_stack\": \"PUNCT\",\n",
      "  \"second_dep_on_buffer\": \"punct\",\n",
      "  \"second_dep_on_stack\": \"root\",\n",
      "  \"second_lemma_on_buffer\": \":\",\n",
      "  \"second_lemma_on_stack\": \"Al\",\n",
      "  \"second_pos_on_buffer\": \"PUNCT\",\n",
      "  \"second_pos_on_stack\": \"PROPN\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Featurizer(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_from_list(l, pos, sent):\n",
    "        if (pos >= 0 and pos < len(l)) or (pos < 0 and len(l) + pos >= 0):\n",
    "            return sent[l[pos]]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @staticmethod\n",
    "    def featurize(c):\n",
    "        s = c.sentence\n",
    "        f = c.features\n",
    "        a = c.arcs\n",
    "        g = TransitionBasedParser.Graph(a)\n",
    "        \n",
    "        lemmas = [\"ROOT\"] + [x['lemma'] for x in f]\n",
    "        pos = [\"ROOT\"] + [x['upostag'] for x in f]\n",
    "        deps = [\"-\"] + [x['deprel'] for x in f]\n",
    "        \n",
    "        outgoing = []\n",
    "        for x in range(len(s)):\n",
    "            for arc in g.starts_with(x):\n",
    "                outgoing.append(arc.right)\n",
    "                continue\n",
    "            outgoing.append(None)\n",
    "        \n",
    "        incoming = []\n",
    "        for x in range(len(s)):\n",
    "            for arc in g.ends_with(x):\n",
    "                incoming.append(arc.left)\n",
    "                continue\n",
    "            incoming.append(None)\n",
    "\n",
    "        return {\n",
    "#             \"first_word_on_stack\": Featurizer.get_from_list(c.stack, -1, s),\n",
    "            \"first_lemma_on_stack\": Featurizer.get_from_list(c.stack, -1, lemmas),\n",
    "            \"first_pos_on_stack\": Featurizer.get_from_list(c.stack, -1, pos),\n",
    "            \"first_dep_on_stack\": Featurizer.get_from_list(c.stack, -1, deps),\n",
    "            \"first_on_stack_incoming\": Featurizer.get_from_list(incoming, c.stack[-1], pos) \n",
    "                if c.stack and incoming[c.stack[-1]] is not None else \"-\",\n",
    "            \"first_on_stack_outgoing\": Featurizer.get_from_list(outgoing, c.stack[-1], pos) \n",
    "                if c.stack and outgoing[c.stack[-1]]is not None else \"-\",\n",
    "            \n",
    "#             \"second_word_on_stack\": Featurizer.get_from_list(c.stack, -2, s),\n",
    "            \"second_lemma_on_stack\": Featurizer.get_from_list(c.stack, -2, lemmas),\n",
    "            \"second_pos_on_stack\": Featurizer.get_from_list(c.stack, -2, pos),\n",
    "            \"second_dep_on_stack\": Featurizer.get_from_list(c.stack, -2, deps),\n",
    "\n",
    "#             \"first_word_on_buffer\": Featurizer.get_from_list(c.buffer, 0, s),\n",
    "            \"first_lemma_on_buffer\": Featurizer.get_from_list(c.buffer, 0, lemmas),\n",
    "            \"first_pos_on_buffer\": Featurizer.get_from_list(c.buffer, 0, pos),\n",
    "            \"first_dep_on_buffer\": Featurizer.get_from_list(c.buffer, 0, deps),\n",
    "             \"first_on_buffer_incoming\": Featurizer.get_from_list(incoming, c.buffer[0], pos) \n",
    "                if c.buffer and incoming[c.buffer[0]] is not None else \"-\",\n",
    "            \"first_on_buffer_outgoing\": Featurizer.get_from_list(outgoing, c.buffer[0], pos) \n",
    "                if c.buffer and outgoing[c.buffer[0]]is not None else \"-\",\n",
    "            \n",
    "#             \"second_word_on_buffer\": Featurizer.get_from_list(c.buffer, 1, s),\n",
    "            \"second_lemma_on_buffer\": Featurizer.get_from_list(c.buffer, 1, lemmas),\n",
    "            \"second_pos_on_buffer\": Featurizer.get_from_list(c.buffer, 1, pos),\n",
    "            \"second_dep_on_buffer\": Featurizer.get_from_list(c.buffer, 1, deps),\n",
    "        }\n",
    "           \n",
    "# for i, x in enumerate(configs):\n",
    "#     if i > 1:\n",
    "#         break\n",
    "#     print(json.dumps(Featurizer.featurize(x), indent=2, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus and turn the configurations into feature dicts. \n",
    "\n",
    "This can take up to 10-15 minutes, depending on the size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:32:12.607748Z",
     "start_time": "2017-12-10T18:20:41.908245Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "y = []\n",
    "\n",
    "for i, (transition, label, config) in enumerate(\n",
    "        ConfigurationExtractor.generate_configurations(\n",
    "            ConllTreeBankReader.parse(corpus))):\n",
    "    if not config or transition == \"INIT\":\n",
    "        continue\n",
    "        \n",
    "    configs.append(config)\n",
    "    \n",
    "    if not label:\n",
    "        label = \"\"\n",
    "        \n",
    "    y.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:36:08.277700Z",
     "start_time": "2017-12-10T18:34:11.797676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dicts = [Featurizer.featurize(x) for x in configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create the actual numpy feature vectors using a dict vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:37:48.826258Z",
     "start_time": "2017-12-10T18:37:35.162637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Todo: Shuffle data\n",
    "\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-valdation to find the best classifier\n",
    "\n",
    "### Warning: This might take hours :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T21:23:59.079104Z",
     "start_time": "2017-12-10T18:41:09.695356Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "Avg. accuracy: 0.2496 (+/- 0.0011)\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.8008 (+/- 0.0045)\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Avg. accuracy: 0.7896 (+/- 0.0056)\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Avg. accuracy: 0.7155 (+/- 0.0060)\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.8207 (+/- 0.0052)\n",
      "\n",
      " Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=100, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.7415 (+/- 0.0071)\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py\", line 1684, in _fit_and_score\n    test_score = _score(estimator, X_test, y_test, scorer)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py\", line 1741, in _score\n    score = scorer(estimator, X_test, y_test)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/scorer.py\", line 91, in __call__\n    y_pred = estimator.predict(X)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/classification.py\", line 145, in predict\n    neigh_dist, neigh_ind = self.kneighbors(X)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/base.py\", line 353, in kneighbors\n    n_jobs=n_jobs, squared=True)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\", line 1240, in pairwise_distances\n    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\", line 1083, in _parallel_pairwise\n    return func(X, Y, **kwds)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\", line 245, in euclidean_distances\n    distances = safe_sparse_dot(X, Y.T, dense_output=True)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\", line 184, in safe_sparse_dot\n    ret = a * b\n  File \"/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\", line 369, in __mul__\n    return self._mul_sparse_matrix(other)\n  File \"/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py\", line 540, in _mul_sparse_matrix\n    indices = np.empty(nnz, dtype=idx_dtype)\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Sun Dec 10 19:49:42 2017\nPID: 347                                Python 3.6.2: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), <382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], make_scorer(accuracy_score), memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), array([    0,     1,     2, ..., 41462, 41468, 41473]), 0, None, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), <382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], make_scorer(accuracy_score), memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), array([    0,     1,     2, ..., 41462, 41468, 41473]), 0, None, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], scorer=make_scorer(accuracy_score), train=memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), test=array([    0,     1,     2, ..., 41462, 41468, 41473]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, error_score='raise')\n   1679                              \" numeric value. (Hint: if using 'raise', please\"\n   1680                              \" make sure that it has been spelled correctly.)\"\n   1681                              )\n   1682 \n   1683     else:\n-> 1684         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance')\n        X_test = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        y_test = ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...]\n        scorer = make_scorer(accuracy_score)\n   1685         if return_train_score:\n   1686             train_score = _score(estimator, X_train, y_train, scorer)\n   1687 \n   1688     scoring_time = time.time() - start_time\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py in _score(estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X_test=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, y_test=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], scorer=make_scorer(accuracy_score))\n   1736 def _score(estimator, X_test, y_test, scorer):\n   1737     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n   1738     if y_test is None:\n   1739         score = scorer(estimator, X_test)\n   1740     else:\n-> 1741         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(accuracy_score)\n        estimator = KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance')\n        X_test = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        y_test = ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...]\n   1742     if hasattr(score, 'item'):\n   1743         try:\n   1744             # e.g. unwrap memmapped scalars\n   1745             score = score.item()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/scorer.py in __call__(self=make_scorer(accuracy_score), estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, y_true=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], sample_weight=None)\n     86         score : float\n     87             Score function applied to prediction of estimator on X.\n     88         \"\"\"\n     89         super(_PredictScorer, self).__call__(estimator, X, y_true,\n     90                                              sample_weight=sample_weight)\n---> 91         y_pred = estimator.predict(X)\n        y_pred = undefined\n        estimator.predict = <bound method KNeighborsClassifier.predict of KN...neighbors=5, p=2,\n           weights='distance')>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n     92         if sample_weight is not None:\n     93             return self._sign * self._score_func(y_true, y_pred,\n     94                                                  sample_weight=sample_weight,\n     95                                                  **self._kwargs)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/classification.py in predict(self=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>)\n    140         y : array of shape [n_samples] or [n_samples, n_outputs]\n    141             Class labels for each data sample.\n    142         \"\"\"\n    143         X = check_array(X, accept_sparse='csr')\n    144 \n--> 145         neigh_dist, neigh_ind = self.kneighbors(X)\n        neigh_dist = undefined\n        neigh_ind = undefined\n        self.kneighbors = <bound method KNeighborsMixin.kneighbors of KNei...neighbors=5, p=2,\n           weights='distance')>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n    146 \n    147         classes_ = self.classes_\n    148         _y = self._y\n    149         if not self.outputs_2d_:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/base.py in kneighbors(self=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, n_neighbors=5, return_distance=True)\n    348         n_jobs = _get_n_jobs(self.n_jobs)\n    349         if self._fit_method == 'brute':\n    350             # for efficiency, use squared euclidean distances\n    351             if self.effective_metric_ == 'euclidean':\n    352                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n--> 353                                           n_jobs=n_jobs, squared=True)\n        n_jobs = 1\n    354             else:\n    355                 dist = pairwise_distances(\n    356                     X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,\n    357                     **self.effective_metric_params_)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, metric='euclidean', n_jobs=1, **kwds={'squared': True})\n   1235         if n_jobs == 1 and X is Y:\n   1236             return distance.squareform(distance.pdist(X, metric=metric,\n   1237                                                       **kwds))\n   1238         func = partial(distance.cdist, metric=metric, **kwds)\n   1239 \n-> 1240     return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y = <344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        func = <function euclidean_distances>\n        n_jobs = 1\n        kwds = {'squared': True}\n   1241 \n   1242 \n   1243 # These distances recquire boolean arrays, when using scipy.spatial.distance\n   1244 PAIRWISE_BOOLEAN_FUNCTIONS = [\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, func=<function euclidean_distances>, n_jobs=1, **kwds={'squared': True})\n   1078     if Y is None:\n   1079         Y = X\n   1080 \n   1081     if n_jobs == 1:\n   1082         # Special case to avoid picklability checks in delayed\n-> 1083         return func(X, Y, **kwds)\n        func = <function euclidean_distances>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y = <344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        kwds = {'squared': True}\n   1084 \n   1085     # TODO: in some cases, backend='threading' may be appropriate\n   1086     fd = delayed(func)\n   1087     ret = Parallel(n_jobs=n_jobs, verbose=0)(\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, Y_norm_squared=None, squared=True, X_norm_squared=None)\n    240             raise ValueError(\n    241                 \"Incompatible dimensions for Y and Y_norm_squared\")\n    242     else:\n    243         YY = row_norms(Y, squared=True)[np.newaxis, :]\n    244 \n--> 245     distances = safe_sparse_dot(X, Y.T, dense_output=True)\n        distances = undefined\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y.T = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    246     distances *= -2\n    247     distances += XX\n    248     distances += YY\n    249     np.maximum(distances, 0, out=distances)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py in safe_sparse_dot(a=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, b=<54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>, dense_output=True)\n    179 \n    180     Uses BLAS GEMM as replacement for numpy.dot where possible\n    181     to avoid unnecessary copies.\n    182     \"\"\"\n    183     if issparse(a) or issparse(b):\n--> 184         ret = a * b\n        ret = undefined\n        a = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        b = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    185         if dense_output and hasattr(ret, \"toarray\"):\n    186             ret = ret.toarray()\n    187         return ret\n    188     else:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py in __mul__(self=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, other=<54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>)\n    364             return self._mul_scalar(other)\n    365 \n    366         if issparse(other):\n    367             if self.shape[1] != other.shape[0]:\n    368                 raise ValueError('dimension mismatch')\n--> 369             return self._mul_sparse_matrix(other)\n        self._mul_sparse_matrix = <bound method _cs_matrix._mul_sparse_matrix of <...stored elements in Compressed Sparse Row format>>\n        other = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    370 \n    371         # If it's a list or whatever, treat it like a matrix\n    372         other_a = np.asanyarray(other)\n    373 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py in _mul_sparse_matrix(self=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, other=<54043x344042 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)\n    535         nnz = indptr[-1]\n    536         idx_dtype = get_index_dtype((self.indptr, self.indices,\n    537                                      other.indptr, other.indices),\n    538                                     maxval=nnz)\n    539         indptr = np.asarray(indptr, dtype=idx_dtype)\n--> 540         indices = np.empty(nnz, dtype=idx_dtype)\n        indices = undefined\n        nnz = 11853242368\n        idx_dtype = <class 'numpy.int64'>\n    541         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))\n    542 \n    543         fn = getattr(_sparsetools, self.format + '_matmat_pass2')\n    544         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),\n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Sun Dec 10 19:49:42 2017\nPID: 347                                Python 3.6.2: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), <382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], make_scorer(accuracy_score), memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), array([    0,     1,     2, ..., 41462, 41468, 41473]), 0, None, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), <382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], make_scorer(accuracy_score), memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), array([    0,     1,     2, ..., 41462, 41468, 41473]), 0, None, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py in _fit_and_score(estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<382271x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], scorer=make_scorer(accuracy_score), train=memmap([ 37125,  37129,  37130, ..., 382268, 382269, 382270]), test=array([    0,     1,     2, ..., 41462, 41468, 41473]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, error_score='raise')\n   1679                              \" numeric value. (Hint: if using 'raise', please\"\n   1680                              \" make sure that it has been spelled correctly.)\"\n   1681                              )\n   1682 \n   1683     else:\n-> 1684         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance')\n        X_test = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        y_test = ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...]\n        scorer = make_scorer(accuracy_score)\n   1685         if return_train_score:\n   1686             train_score = _score(estimator, X_train, y_train, scorer)\n   1687 \n   1688     scoring_time = time.time() - start_time\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py in _score(estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X_test=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, y_test=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], scorer=make_scorer(accuracy_score))\n   1736 def _score(estimator, X_test, y_test, scorer):\n   1737     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n   1738     if y_test is None:\n   1739         score = scorer(estimator, X_test)\n   1740     else:\n-> 1741         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(accuracy_score)\n        estimator = KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance')\n        X_test = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        y_test = ['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...]\n   1742     if hasattr(score, 'item'):\n   1743         try:\n   1744             # e.g. unwrap memmapped scalars\n   1745             score = score.item()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/scorer.py in __call__(self=make_scorer(accuracy_score), estimator=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, y_true=['RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'SH', 'LA', 'SH', 'LA', 'RE', 'RA', 'RA', 'RA', 'RE', 'RA', 'RE', 'RA', 'RE', 'RA', ...], sample_weight=None)\n     86         score : float\n     87             Score function applied to prediction of estimator on X.\n     88         \"\"\"\n     89         super(_PredictScorer, self).__call__(estimator, X, y_true,\n     90                                              sample_weight=sample_weight)\n---> 91         y_pred = estimator.predict(X)\n        y_pred = undefined\n        estimator.predict = <bound method KNeighborsClassifier.predict of KN...neighbors=5, p=2,\n           weights='distance')>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n     92         if sample_weight is not None:\n     93             return self._sign * self._score_func(y_true, y_pred,\n     94                                                  sample_weight=sample_weight,\n     95                                                  **self._kwargs)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/classification.py in predict(self=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>)\n    140         y : array of shape [n_samples] or [n_samples, n_outputs]\n    141             Class labels for each data sample.\n    142         \"\"\"\n    143         X = check_array(X, accept_sparse='csr')\n    144 \n--> 145         neigh_dist, neigh_ind = self.kneighbors(X)\n        neigh_dist = undefined\n        neigh_ind = undefined\n        self.kneighbors = <bound method KNeighborsMixin.kneighbors of KNei...neighbors=5, p=2,\n           weights='distance')>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n    146 \n    147         classes_ = self.classes_\n    148         _y = self._y\n    149         if not self.outputs_2d_:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/base.py in kneighbors(self=KNeighborsClassifier(algorithm='auto', leaf_size..._neighbors=5, p=2,\n           weights='distance'), X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, n_neighbors=5, return_distance=True)\n    348         n_jobs = _get_n_jobs(self.n_jobs)\n    349         if self._fit_method == 'brute':\n    350             # for efficiency, use squared euclidean distances\n    351             if self.effective_metric_ == 'euclidean':\n    352                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n--> 353                                           n_jobs=n_jobs, squared=True)\n        n_jobs = 1\n    354             else:\n    355                 dist = pairwise_distances(\n    356                     X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,\n    357                     **self.effective_metric_params_)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, metric='euclidean', n_jobs=1, **kwds={'squared': True})\n   1235         if n_jobs == 1 and X is Y:\n   1236             return distance.squareform(distance.pdist(X, metric=metric,\n   1237                                                       **kwds))\n   1238         func = partial(distance.cdist, metric=metric, **kwds)\n   1239 \n-> 1240     return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y = <344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        func = <function euclidean_distances>\n        n_jobs = 1\n        kwds = {'squared': True}\n   1241 \n   1242 \n   1243 # These distances recquire boolean arrays, when using scipy.spatial.distance\n   1244 PAIRWISE_BOOLEAN_FUNCTIONS = [\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, func=<function euclidean_distances>, n_jobs=1, **kwds={'squared': True})\n   1078     if Y is None:\n   1079         Y = X\n   1080 \n   1081     if n_jobs == 1:\n   1082         # Special case to avoid picklability checks in delayed\n-> 1083         return func(X, Y, **kwds)\n        func = <function euclidean_distances>\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y = <344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        kwds = {'squared': True}\n   1084 \n   1085     # TODO: in some cases, backend='threading' may be appropriate\n   1086     fd = delayed(func)\n   1087     ret = Parallel(n_jobs=n_jobs, verbose=0)(\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, Y=<344042x54043 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, Y_norm_squared=None, squared=True, X_norm_squared=None)\n    240             raise ValueError(\n    241                 \"Incompatible dimensions for Y and Y_norm_squared\")\n    242     else:\n    243         YY = row_norms(Y, squared=True)[np.newaxis, :]\n    244 \n--> 245     distances = safe_sparse_dot(X, Y.T, dense_output=True)\n        distances = undefined\n        X = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        Y.T = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    246     distances *= -2\n    247     distances += XX\n    248     distances += YY\n    249     np.maximum(distances, 0, out=distances)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py in safe_sparse_dot(a=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, b=<54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>, dense_output=True)\n    179 \n    180     Uses BLAS GEMM as replacement for numpy.dot where possible\n    181     to avoid unnecessary copies.\n    182     \"\"\"\n    183     if issparse(a) or issparse(b):\n--> 184         ret = a * b\n        ret = undefined\n        a = <38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>\n        b = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    185         if dense_output and hasattr(ret, \"toarray\"):\n    186             ret = ret.toarray()\n    187         return ret\n    188     else:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py in __mul__(self=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, other=<54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>)\n    364             return self._mul_scalar(other)\n    365 \n    366         if issparse(other):\n    367             if self.shape[1] != other.shape[0]:\n    368                 raise ValueError('dimension mismatch')\n--> 369             return self._mul_sparse_matrix(other)\n        self._mul_sparse_matrix = <bound method _cs_matrix._mul_sparse_matrix of <...stored elements in Compressed Sparse Row format>>\n        other = <54043x344042 sparse matrix of type '<class 'num...ored elements in Compressed Sparse Column format>\n    370 \n    371         # If it's a list or whatever, treat it like a matrix\n    372         other_a = np.asanyarray(other)\n    373 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py in _mul_sparse_matrix(self=<38229x54043 sparse matrix of type '<class 'nump... stored elements in Compressed Sparse Row format>, other=<54043x344042 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)\n    535         nnz = indptr[-1]\n    536         idx_dtype = get_index_dtype((self.indptr, self.indices,\n    537                                      other.indptr, other.indices),\n    538                                     maxval=nnz)\n    539         indptr = np.asarray(indptr, dtype=idx_dtype)\n--> 540         indices = np.empty(nnz, dtype=idx_dtype)\n        indices = undefined\n        nnz = 11853242368\n        idx_dtype = <class 'numpy.int64'>\n    541         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))\n    542 \n    543         fn = getattr(_sparsetools, self.format + '_matmat_pass2')\n    544         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),\n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-9e95b40dcec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Train 10 times, each time holding 10% of the data out to validate on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# The overall accuracy is then an average of the 10 training results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Avg. accuracy: %0.4f (+/- %0.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1569\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1571\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m\"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mWindowsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;31m# Workaround  occasional \"[Error 5] Access is denied\" issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;31m# when trying to terminate a process under windows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers_to_test = [\n",
    "    DummyClassifier(strategy='uniform'),\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    BernoulliNB(),\n",
    "    RandomForestClassifier(),\n",
    "    Perceptron(n_iter=100),\n",
    "#     KNeighborsClassifier(weights=\"distance\"), # <- theres sth wrong with it Oo\n",
    "]\n",
    "\n",
    "from sklearn import cross_validation\n",
    "for clf in classifiers_to_test:\n",
    "    print(\"\\n\", clf)\n",
    "    # Train 10 times, each time holding 10% of the data out to validate on.\n",
    "    # The overall accuracy is then an average of the 10 training results.\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    print(\"Avg. accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results (accuracy)\n",
    "\n",
    "| Run | Baseline | LogisticRegression | SVC | Bernoulli Naive Bayes | Random Forest  | Perceptron | Nearest Neighbor |\n",
    "|-----|----------|--------------------|-----|-----------------------|----------------|------------|------------------|\n",
    "|  1  | 0.2496  | 0.8008 | 0.7896 | 0.7155 | **0.8207** |  0.7415 | | |\n",
    "\n",
    "\n",
    "Run 1: Features (example): \n",
    "```\n",
    "[\"first_dep_on_buffer\", \"first_dep_on_stack\", \"first_lemma_on_buffer\", \"first_lemma_on_stack\", \"first_on_buffer_incoming\", \"first_on_buffer_outgoing\", \"first_on_stack_incoming\", \"first_on_stack_outgoing\",  \"first_pos_on_buffer\", \"first_pos_on_stack\", \"second_dep_on_buffer\", \"second_dep_on_stack\", \"second_lemma_on_buffer\",  \"second_lemma_on_stack\", \"second_pos_on_buffer\", \"second_pos_on_stack\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T21:28:03.701365Z",
     "start_time": "2017-12-10T21:24:16.285416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(n_jobs=-1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FINAL PARSER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:44:19.215918Z",
     "start_time": "2017-12-12T07:44:19.206129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = v\n",
    "featurizer = Featurizer\n",
    "classifier = clf\n",
    "\n",
    "parser = TransitionBasedParser([\"label\"], ml_data={\n",
    "    \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T07:44:45.961184Z",
     "start_time": "2017-12-12T07:44:45.898883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[OrderedDict([('id', 1), ('form', 'He'), ('lemma', 'he'), ('upostag', 'PRON'), ('xpostag', 'PRON'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 2), ('form', 'worked'), ('lemma', 'work'), ('upostag', 'VERB'), ('xpostag', 'VERB'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 3), ('form', 'for'), ('lemma', 'for'), ('upostag', 'ADP'), ('xpostag', 'ADP'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 4), ('form', 'the'), ('lemma', 'the'), ('upostag', 'DET'), ('xpostag', 'DET'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 5), ('form', 'BBC'), ('lemma', 'BBC'), ('upostag', 'PROPN'), ('xpostag', 'PROPN'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 6), ('form', 'for'), ('lemma', 'for'), ('upostag', 'ADP'), ('xpostag', 'ADP'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 7), ('form', 'a'), ('lemma', 'a'), ('upostag', 'DET'), ('xpostag', 'DET'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 8), ('form', 'decade'), ('lemma', 'decade'), ('upostag', 'NOUN'), ('xpostag', 'NOUN'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)]), OrderedDict([('id', 9), ('form', '.'), ('lemma', '.'), ('upostag', 'PUNCT'), ('xpostag', 'PUNCT'), ('feats', None), ('head', None), ('deprel', '_'), ('deps', None), ('misc', None)])]]\n",
      "Initial configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['ROOT']\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['He']\n",
      "  buffer=['worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['worked']\n",
      "  buffer=['for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['for']\n",
      "  buffer=['the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['the']\n",
      "  buffer=['BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC']\n",
      "  buffer=['for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for']\n",
      "  buffer=['a', 'decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "RA\n",
      "Right arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for', 'a']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC', 'for']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['BBC']\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['decade', '.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['decade']\n",
      "  buffer=['.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['.']\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC), (. --|label|--> decade)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "LA\n",
      "Left arc\n",
      "Fallback Shift\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['.']\n",
      "  buffer=[]\n",
      "  arcs=[(He --|label|--> ROOT), (worked --|label|--> He), (for --|label|--> worked), (the --|label|--> for), (BBC --|label|--> the), (BBC --|label|--> for), (for --|label|--> a), (decade --|label|--> a), (decade --|label|--> for), (decade --|label|--> BBC), (. --|label|--> decade)]\n",
      "</CONFIGURATION>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(He --|label|--> ROOT),\n",
       " (worked --|label|--> He),\n",
       " (for --|label|--> worked),\n",
       " (the --|label|--> for),\n",
       " (BBC --|label|--> the),\n",
       " (BBC --|label|--> for),\n",
       " (for --|label|--> a),\n",
       " (decade --|label|--> a),\n",
       " (decade --|label|--> for),\n",
       " (decade --|label|--> BBC),\n",
       " (. --|label|--> decade)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from conllu.parser import parse\n",
    "\n",
    "sent = \"\"\"\n",
    "1\tHe\the\tPRON\tPRON\t_\t_\t_\t_\t_\n",
    "2\tworked\twork\tVERB\tVERB\t_\t_\t_\t_\t_\n",
    "3\tfor\tfor\tADP\tADP\t_\t_\t_\t_\t_\n",
    "4\tthe\tthe\tDET\tDET\t_\t_\t_\t_\t_\n",
    "5\tBBC\tBBC\tPROPN\tPROPN\t_\t_\t_\t_\t_\n",
    "6\tfor\tfor\tADP\tADP\t_\t_\t_\t_\t_\n",
    "7\ta\ta\tDET\tDET\t_\t_\t_\t_\t_\n",
    "8\tdecade\tdecade\tNOUN\tNOUN\t_\t_\t_\t_\t_\n",
    "9\t.\t.\tPUNCT\tPUNCT\t_\t_\t_\t_\t_\n",
    "\"\"\"\n",
    "\n",
    "features_test = parse(sent)\n",
    "\n",
    "print(features_test)\n",
    "\n",
    "parser.parse([\"ROOT\"] + [x['form'] for x in features_test[0]], features_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Constituency Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T18:16:23.963525Z",
     "start_time": "2017-12-08T18:15:06.285Z"
    },
    "code_folding": [
     0,
     13,
     57
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rule(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                lhs: str,\n",
    "                rhs: List[str],\n",
    "                prob: float=1.0):\n",
    "        self.lhs = lhs\n",
    "        self.rhs = rhs\n",
    "        self.prob = prob\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"'{self.lhs}' -> {self.rhs} [{self.prob}]\"\n",
    "\n",
    "class Grammar(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 start: str,\n",
    "                 rules: Iterable[Rule],\n",
    "                 non_terminals: Iterable[str]=None,\n",
    "                 terminals: Iterable[str]=None):\n",
    "        # Leave nonterminals and terminals empty to infer them from the rules\n",
    "        self.start = start\n",
    "        self.rules = set(rules)\n",
    "        \n",
    "        if non_terminals:\n",
    "            self.non_terminals = set(non_terminals)\n",
    "        if terminals:\n",
    "            self.terminals = set(terminals)\n",
    "            \n",
    "        if non_terminals is None or terminals is None:\n",
    "            self.non_terminals = set()\n",
    "            self.terminals = set()\n",
    "            \n",
    "            for rule in self.rules:\n",
    "                self.non_terminals.add(rule.lhs)\n",
    "                if len(rule.rhs) == 1:\n",
    "                    self.terminals.add(rule.rhs[0])\n",
    "                elif len(rule.rhs) == 2:\n",
    "                    self.non_terminals.add(rule.rhs[0])\n",
    "                    self.non_terminals.add(rule.rhs[1])\n",
    "                else:\n",
    "                    raise ValueError(\"Only rules in Chomsky Normal Form supported.\")\n",
    "        \n",
    "        self.non_terminals.add(start)\n",
    "        \n",
    "    def get_lhs_for_rhs(self, rhs: Sequence[str]) -> Set[str]:\n",
    "        rhs = list(rhs)\n",
    "        x = set(rule.lhs for rule in\n",
    "            filter(lambda rule: rule.rhs == rhs, self.rules)\n",
    "        )\n",
    "        if x:\n",
    "            print(list(filter(lambda rule: rule.rhs == rhs, self.rules)))\n",
    "        return x\n",
    "                \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"({self.start}, {self.non_terminals}, {self.terminals}, {self.rules})\"\n",
    "\n",
    "class CYKParser(object):\n",
    "    \n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "        \n",
    "    def draw_table(self, table: List[List[Set[str]]]) -> str:\n",
    "        ret = []\n",
    "        \n",
    "        for i, row in enumerate(table):\n",
    "            row_ret = \"\"\n",
    "            for j, cell in enumerate(row):\n",
    "                row_ret += f\"[{i},{j}: {cell}]\"\n",
    "            ret.append(row_ret)\n",
    "        \n",
    "        return \"\\n\".join(ret)\n",
    "        \n",
    "    def recognize(self, sentence: Sequence[str]) -> bool:\n",
    "        # Implemented like Figure 13.10 in J&M p.474\n",
    "        \n",
    "        sentence = list(sentence)\n",
    "        \n",
    "        # Fill the table, a list of lists.\n",
    "        table = [\n",
    "            [set() for _ in range(len(sentence))]\n",
    "            for _ in range(len(sentence))\n",
    "        ]\n",
    "                \n",
    "        # Iterate word by word. \n",
    "        # Note: The indices may be off by one compared to the \n",
    "        # pseudo code in the book due to pythons lists etc.\n",
    "        for j in range(len(sentence)):\n",
    "            # Put the POS tag for the current terminal in the diagonal cells\n",
    "            print()\n",
    "            table[j][j] = self.grammar.get_lhs_for_rhs([sentence[j]])\n",
    "\n",
    "            for i in range(j, -1, -1):\n",
    "                for k in range(i, j):\n",
    "                    # NT1 describes the span from i to k,\n",
    "                    # NT2 the one from k to j\n",
    "                    # If we find something that goes X -> NT1 NT2,\n",
    "                    # we can conver the span i to j!\n",
    "                    print(f\"j={j} | i={i}\")\n",
    "\n",
    "                    non_terminals_1 = table[i][k]\n",
    "                    non_terminals_2 = table[k + 1][j]\n",
    "                    \n",
    "                    \n",
    "                    for rhs in itertools.product(non_terminals_1, non_terminals_2):\n",
    "                        # Find a left hand side from rules that produce\n",
    "                        # both non-terminals. Add them (set: union) to the table.\n",
    "                        table[i][j] = table[i][j] | self.grammar.get_lhs_for_rhs(rhs)\n",
    "\n",
    "            \n",
    "        print(self.draw_table(table))\n",
    "        return self.grammar.start in table[0][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T18:16:24.015343Z",
     "start_time": "2017-12-08T18:15:06.755Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules = {\n",
    "    Rule(\"SS\", \"S Punct\".split()),\n",
    "    Rule(\"S\", \"NP VP\".split()),\n",
    "    Rule(\"S\", \"Pron VP\".split()),\n",
    "    Rule(\"NP\", \"Det Noun\".split()),\n",
    "    Rule(\"VP\", \"Verb VP1\".split()),\n",
    "    Rule(\"VP1\", \"PP PP\".split()),\n",
    "    Rule(\"PP\", \"Prep NP\".split()),\n",
    "    Rule(\"PP\", \"Prep Pron\".split()),\n",
    "    Rule(\"Noun\", [\"BBC\"]),\n",
    "    Rule(\"Noun\", [\"decade\"]),\n",
    "    Rule(\"Verb\", [\"worked\"]),\n",
    "    Rule(\"Pron\", [\"He\"]),\n",
    "    Rule(\"Det\", [\"the\"]),\n",
    "    Rule(\"Det\", [\"a\"]),\n",
    "    Rule(\"Prep\", [\"for\"]),\n",
    "    Rule(\"Punct\", [\".\"])\n",
    "}\n",
    "\n",
    "g = Grammar(\"SS\", rules)\n",
    "\n",
    "p = CYKParser(g)\n",
    "\n",
    "print(p.recognize(\"He worked for the BBC for a decade .\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
