{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:53:56.712491Z",
     "start_time": "2017-12-08T17:53:56.691245Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global dependencies here\n",
    "from typing import Dict, List, Set, Iterable, Sequence, Tuple\n",
    "from collections import deque\n",
    "import itertools\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc-eager transition-based parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:54:03.818440Z",
     "start_time": "2017-12-08T17:53:58.849822Z"
    },
    "code_folding": [
     0,
     2,
     19,
     35,
     151,
     175
    ]
   },
   "outputs": [],
   "source": [
    "class TransitionBasedParser(object):\n",
    "    \n",
    "    class Arc(object):\n",
    "        \n",
    "        def __init__(self, left_word: int, label: str, right_word: int,\n",
    "                     sentence: List[str], features=None):\n",
    "            # sentence is required to start with \"ROOT\"\n",
    "            self.left = left_word\n",
    "            self.label = label\n",
    "            self.right = right_word\n",
    "            self.sentence = sentence\n",
    "            self.features = features or []\n",
    "            \n",
    "        def __repr__(self):\n",
    "            left = self.sentence[self.left]\n",
    "            right = self.sentence[self.right]\n",
    "            return f\"({left} --|{self.label}|--> {right})\"\n",
    "\n",
    "        \n",
    "    class Configuration(object):\n",
    "        \n",
    "        def __init__(self, stack: List[int], buffer: List[int],\n",
    "                     arcs: List, sentence: List[str]):\n",
    "            self.stack = stack\n",
    "            self.buffer = deque(buffer)\n",
    "            self.arcs = arcs\n",
    "            self.sentence = sentence\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            stack = [self.sentence[i] for i in self.stack]\n",
    "            buffer = [self.sentence[i] for i in self.buffer]\n",
    "\n",
    "            return f\"<CONFIGURATION>\\n  stack={stack}\\n  buffer={buffer}\\n  arcs={self.arcs}\\n</CONFIGURATION>\"\n",
    "        \n",
    "        \n",
    "    class Graph(object):\n",
    "        \n",
    "        def __init__(self, arcs: List=None):\n",
    "            self.arcs = arcs or []\n",
    "            \n",
    "        def starts_with(self, index):\n",
    "            return filter(lambda arc: arc.left == index, self.arcs)\n",
    "        \n",
    "        def contains_arc(self, left: int, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right, \n",
    "                           self.arcs)\n",
    "            \n",
    "        def contains_arc_label(self, left: int, label: str, right: int):\n",
    "             return filter(lambda arc:\n",
    "                               arc.left == left\n",
    "                               and arc.right == right\n",
    "                               and arc.label == label, \n",
    "                           self.arcs)\n",
    "        \n",
    "        def __repr__(self) -> str:\n",
    "            return str(self.arcs)\n",
    "        \n",
    "        def subset(self, other):\n",
    "            # Are all the arcs in the other graph in this one?\n",
    "            a = set([(a.left, a.label, a.right) for a in self.arcs])\n",
    "            b = set([(a.left, a.label, a.right) for a in other])\n",
    "\n",
    "            return b.difference(a)\n",
    "                    \n",
    "\n",
    "    \n",
    "    def __init__(self, labels: List[str], ml_data=None):\n",
    "        self.labels = labels\n",
    "        self.ml_data = ml_data\n",
    "    \n",
    "    def parse(self, sentence: List[str]):\n",
    "        # Accepts a list of tokens\n",
    "        configuration = self._initialization(sentence)\n",
    "        print(\"Initial configuration:\")\n",
    "        print(configuration)\n",
    "\n",
    "        # Keep making transitions until the termination\n",
    "        # requirement is met.\n",
    "        while not self._check_termination(configuration):\n",
    "            print()\n",
    "            configuration = self._make_transition(configuration)\n",
    "            print(\"New configuration:\")\n",
    "            print(configuration)\n",
    "        \n",
    "        return configuration.arcs\n",
    "    \n",
    "    def _make_transition(self, c):\n",
    "        # Get features for the configuration\n",
    "        #     \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "\n",
    "        features = self.ml_data['feat'].featurize(c)\n",
    "        \n",
    "        # Now turn them into a vector\n",
    "        vector = self.ml_data['vec'].transform(features)\n",
    "        \n",
    "        prediction = self.ml_data['clf'].predict(vector)\n",
    "        if prediction == \"LA\":\n",
    "            new_configuration = self._left_arc(self.labels[0], c)\n",
    "        elif prediction == \"LA\":\n",
    "            new_configuration = self._right_arc(self.labels[0], c)\n",
    "        elif prediction == \"SH\":\n",
    "            new_configuration = self._shift(c)\n",
    "        else:\n",
    "            new_configuration = self._reduce(c)\n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _make_transition_naive(self, c):\n",
    "        # Two big questions:\n",
    "        # How to chose the correct label?\n",
    "        # How to chose the correct transition\n",
    "        label = self.labels[0]\n",
    "        new_configuration = None\n",
    "             \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._reduce(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._left_arc(label, c)\n",
    "\n",
    "        if not new_configuration:\n",
    "            new_configuration = self._shift(c)\n",
    "            \n",
    "        if not new_configuration:\n",
    "            new_configuration = self._right_arc(label, c)  \n",
    "        \n",
    "        return new_configuration\n",
    "    \n",
    "    def _left_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "                \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Remove from stack\n",
    "        w_i = c.stack.pop()\n",
    "        \n",
    "        # Keep on buffer\n",
    "        w_j = c.buffer[0]\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_j -> w_i\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_j, label=label, right_word=w_i, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _right_arc(self, label: str, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Keep on stack\n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Remove from buffer\n",
    "        w_j = c.buffer.popleft()\n",
    "        \n",
    "        if w_i is None or w_j is None:\n",
    "            return None\n",
    "        \n",
    "        # Create arc: w_i -> w_j\n",
    "        arc = TransitionBasedParser.Arc(left_word=w_i, label=label, right_word=w_j, sentence=c.sentence)\n",
    "        c.arcs.append(arc)\n",
    "        \n",
    "        # Append wj to stack\n",
    "        c.stack.append(w_j)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _reduce(self, c):\n",
    "        c = deepcopy(c)\n",
    "        \n",
    "        if len(c.stack) == 0:\n",
    "            return None\n",
    "        \n",
    "        w_i = c.stack[-1]\n",
    "        \n",
    "        # Seach for an arc leading to w_i\n",
    "        valid_arc = list(filter(lambda arc: arc.right == w_i, c.arcs))\n",
    "        \n",
    "        if not valid_arc:\n",
    "            return None\n",
    "        \n",
    "        # Remove w_i from the stack\n",
    "        c.stack.pop()\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def _shift(self, c):        \n",
    "        if len(c.buffer) <= 1:\n",
    "            return None\n",
    "        \n",
    "        c = deepcopy(c)\n",
    "        c.stack.append(c.buffer.popleft())\n",
    "        return c\n",
    "    \n",
    "    def _initialization(self, sentence: List[str]):\n",
    "        return TransitionBasedParser.Configuration(\n",
    "            stack=[0], buffer=list(range(1, len(sentence))),\n",
    "            arcs=[], sentence=sentence)\n",
    "    \n",
    "    def _check_termination(self, c) -> bool:\n",
    "        return not c or not c.buffer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:44.310212Z",
     "start_time": "2017-12-08T17:54:04.020598Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Donload training dataset\n",
    "f = requests.get(\"https://www.dropbox.com/s/nnxvjk1n3wc5sof/ewt-train.conll?dl=1\")\n",
    "\n",
    "corpus = f.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus in the CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:44.578695Z",
     "start_time": "2017-12-08T17:55:44.509181Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from conllu.parser import parse\n",
    "\n",
    "class ConllTreeBankReader(object):\n",
    "        \n",
    "    @staticmethod\n",
    "    def parse(content: str):\n",
    "        # Iterates over text in CONLL format and generates\n",
    "        # Graphs with the arcs of the dependency tree\n",
    "        Arc = TransitionBasedParser.Arc\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "\n",
    "        for sent in parse(content):\n",
    "            sentence_str = [\"ROOT\"] + [t['xpostag'] for t in sent]\n",
    "            \n",
    "            yield Graph([Arc(\n",
    "                left_word=t['head'],\n",
    "                label=t['deprel'],\n",
    "                right_word=t['id'],\n",
    "                sentence=sentence_str,\n",
    "                features=sent\n",
    "            ) for t in sent])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a dependency graph into a sequence of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:52.017774Z",
     "start_time": "2017-12-08T17:55:51.251208Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConfigurationExtractor(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_configurations(graphs: Sequence[TransitionBasedParser.Graph]):\n",
    "        Config = TransitionBasedParser.Configuration\n",
    "        Graph = TransitionBasedParser.Graph\n",
    "        parser = TransitionBasedParser(None)\n",
    "        \n",
    "        for graph in graphs:\n",
    "            # Skip sentences without arcs\n",
    "            if not graph.arcs:\n",
    "                continue\n",
    "                                \n",
    "            sentence = graph.arcs[0].sentence  \n",
    "            \n",
    "            \"\"\" Generate initial configuration:\n",
    "            \n",
    "            For every instance (S_d , G_d) ∈ D, we ﬁrst construct a transition sequence \n",
    "            C_(0,m) = (c_0, c_1, ... , c_m) such that\n",
    "\n",
    "                1. c_0 = c_0(S_d), <====\n",
    "                2. G_d = (V_d, A_c_m).\n",
    "            \"\"\"\n",
    "            \n",
    "            configuration = parser._initialization(sentence)\n",
    "            yield \"INIT\", None, configuration\n",
    "            \n",
    "            \"\"\" Use the oracle function to generate all\n",
    "            non-terminal configurations:\n",
    "        \n",
    "            See: Algorithm 1 in:\n",
    "            \"A Dynamic Oracle for Arc-Eager Dependency Parsing\"\n",
    "            \"\"\"\n",
    "            \n",
    "            i = 500\n",
    "            while not parser._check_termination(configuration):\n",
    "                # assert not graph.subset(configuration.arcs)\n",
    "                \n",
    "                # Make sure not to end up in an infinite loop\n",
    "                i -= 1\n",
    "                if i <= 0:\n",
    "                    break\n",
    "                    \n",
    "                # For all arcs, we need the buffer and stack to be not empty\n",
    "                if len(configuration.stack) > 0 and len(configuration.buffer) > 0:\n",
    "                    buffer_0 = configuration.buffer[0]\n",
    "                    stack_0 = configuration.stack[-1]\n",
    "\n",
    "                    # Check for left-arc:\n",
    "                    # if (β[0], r, σ[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(buffer_0, stack_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._left_arc(label, configuration)\n",
    "                        yield \"LA\", label, configuration\n",
    "                        continue\n",
    "\n",
    "                    # Check for right-arc:\n",
    "                    # if (σ[0], r, β[0]) ∈ A_d\n",
    "                    arcs = list(graph.contains_arc(stack_0, buffer_0))\n",
    "                    if arcs:\n",
    "                        assert len(arcs) == 1\n",
    "                        label = arcs[0].label\n",
    "                        configuration = parser._right_arc(label, configuration)\n",
    "                        yield \"RA\", label, configuration\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for reduce\n",
    "                    success = False\n",
    "                    for k in range(stack_0):\n",
    "                        arcs = list(graph.contains_arc(k, buffer_0))\n",
    "                        arcs += list(graph.contains_arc(buffer_0, k))\n",
    "                        if arcs:\n",
    "                            configuration = parser._reduce(configuration)\n",
    "                            yield \"RE\", None, configuration\n",
    "                            success = True\n",
    "                            break\n",
    "                    if success:\n",
    "                        continue\n",
    "                            \n",
    "                # Else: Shift\n",
    "                configuration = parser._shift(configuration)\n",
    "                yield \"SH\", None, configuration\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a configuration into a FeatureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:55:54.961816Z",
     "start_time": "2017-12-08T17:55:54.854303Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Featurizer(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_from_list(l, pos, sent):\n",
    "        if (pos >= 0 and pos < len(l)) or (pos < 0 and len(l) + pos >= 0):\n",
    "            return sent[l[pos]]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @staticmethod\n",
    "    def featurize(c):\n",
    "        # TODO add more features!\n",
    "        s = c.sentence\n",
    "        return {\n",
    "            \"first_word_on_stack\": Featurizer.get_from_list(c.stack, -1, s),\n",
    "            \"second_word_on_stack\": Featurizer.get_from_list(c.stack, -2, s),\n",
    "\n",
    "            \"first_word_on_buffer\": Featurizer.get_from_list(c.buffer, 0, s),\n",
    "            \"second_word_on_buffer\": Featurizer.get_from_list(c.buffer, 1, s),\n",
    "            \"third_word_on_buffer\": Featurizer.get_from_list(c.buffer, 2, s),\n",
    "            \"fourth_word_on_buffer\": Featurizer.get_from_list(c.buffer, 3, s),\n",
    "            \"fifth_word_on_buffer\": Featurizer.get_from_list(c.buffer, 4, s),\n",
    "        }\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T18:09:21.005699Z",
     "start_time": "2017-12-08T17:58:46.094702Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "y = []\n",
    "\n",
    "for i, (transition, label, config) in enumerate(\n",
    "        ConfigurationExtractor.generate_configurations(\n",
    "            ConllTreeBankReader.parse(corpus))):\n",
    "    if not config or transition == \"INIT\":\n",
    "        continue\n",
    "        \n",
    "    configs.append(config)\n",
    "    \n",
    "    if not label:\n",
    "        label = \"\"\n",
    "        \n",
    "    y.append(transition)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform([Featurizer.featurize(x) for x in configs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-valdation to find the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-08T17:37:04.114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "Avg. accuracy: 0.2494 (+/- 0.0005)\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.6979 (+/- 0.0066)\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Avg. accuracy: 0.6756 (+/- 0.0079)\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Avg. accuracy: 0.6567 (+/- 0.0066)\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers_to_test = [\n",
    "    DummyClassifier(strategy='uniform'),\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    BernoulliNB(),\n",
    "    RandomForestClassifier(),\n",
    "#     Perceptron(n_iter=100),\n",
    "#     KNeighborsClassifier(weights=\"distance\"),\n",
    "]\n",
    "\n",
    "from sklearn import cross_validation\n",
    "for clf in classifiers_to_test:\n",
    "    print(\"\\n\", clf)\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    print(\"Avg. accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:36:08.753948Z",
     "start_time": "2017-12-08T17:34:31.608068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FINAL PARSER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T17:37:12.666139Z",
     "start_time": "2017-12-08T17:37:12.631189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=['ROOT']\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[]\n",
      "</CONFIGURATION>\n",
      "\n",
      "New configuration:\n",
      "<CONFIGURATION>\n",
      "  stack=[]\n",
      "  buffer=['He', 'worked', 'for', 'the', 'BBC', 'for', 'a', 'decade', '.']\n",
      "  arcs=[(He --|-|--> ROOT)]\n",
      "</CONFIGURATION>\n",
      "\n",
      "New configuration:\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'arcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-e8cbc9b90744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m })\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROOT He worked for the BBC for a decade .\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-177-bdca998e4b4c>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'arcs'"
     ]
    }
   ],
   "source": [
    "vectorizer = v\n",
    "featurizer = Featurizer\n",
    "classifier = clf\n",
    "\n",
    "parser = TransitionBasedParser([\"-\"], ml_data={\n",
    "    \"vec\": vectorizer, \"feat\": featurizer, \"clf\": classifier\n",
    "})\n",
    "\n",
    "parser.parse(\"ROOT He worked for the BBC for a decade .\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Constituency Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T09:44:14.944542Z",
     "start_time": "2017-12-08T09:43:53.027Z"
    },
    "code_folding": [
     0,
     13,
     57
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rule(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                lhs: str,\n",
    "                rhs: List[str],\n",
    "                prob: float=1.0):\n",
    "        self.lhs = lhs\n",
    "        self.rhs = rhs\n",
    "        self.prob = prob\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"'{self.lhs}' -> {self.rhs} [{self.prob}]\"\n",
    "\n",
    "class Grammar(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 start: str,\n",
    "                 rules: Iterable[Rule],\n",
    "                 non_terminals: Iterable[str]=None,\n",
    "                 terminals: Iterable[str]=None):\n",
    "        # Leave nonterminals and terminals empty to infer them from the rules\n",
    "        self.start = start\n",
    "        self.rules = set(rules)\n",
    "        \n",
    "        if non_terminals:\n",
    "            self.non_terminals = set(non_terminals)\n",
    "        if terminals:\n",
    "            self.terminals = set(terminals)\n",
    "            \n",
    "        if non_terminals is None or terminals is None:\n",
    "            self.non_terminals = set()\n",
    "            self.terminals = set()\n",
    "            \n",
    "            for rule in self.rules:\n",
    "                self.non_terminals.add(rule.lhs)\n",
    "                if len(rule.rhs) == 1:\n",
    "                    self.terminals.add(rule.rhs[0])\n",
    "                elif len(rule.rhs) == 2:\n",
    "                    self.non_terminals.add(rule.rhs[0])\n",
    "                    self.non_terminals.add(rule.rhs[1])\n",
    "                else:\n",
    "                    raise ValueError(\"Only rules in Chomsky Normal Form supported.\")\n",
    "        \n",
    "        self.non_terminals.add(start)\n",
    "        \n",
    "    def get_lhs_for_rhs(self, rhs: Sequence[str]) -> Set[str]:\n",
    "        rhs = list(rhs)\n",
    "        x = set(rule.lhs for rule in\n",
    "            filter(lambda rule: rule.rhs == rhs, self.rules)\n",
    "        )\n",
    "        if x:\n",
    "            print(list(filter(lambda rule: rule.rhs == rhs, self.rules)))\n",
    "        return x\n",
    "                \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"({self.start}, {self.non_terminals}, {self.terminals}, {self.rules})\"\n",
    "\n",
    "class CYKParser(object):\n",
    "    \n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "        \n",
    "    def draw_table(self, table: List[List[Set[str]]]) -> str:\n",
    "        ret = []\n",
    "        \n",
    "        for i, row in enumerate(table):\n",
    "            row_ret = \"\"\n",
    "            for j, cell in enumerate(row):\n",
    "                row_ret += f\"[{i},{j}: {cell}]\"\n",
    "            ret.append(row_ret)\n",
    "        \n",
    "        return \"\\n\".join(ret)\n",
    "        \n",
    "    def recognize(self, sentence: Sequence[str]) -> bool:\n",
    "        # Implemented like Figure 13.10 in J&M p.474\n",
    "        \n",
    "        sentence = list(sentence)\n",
    "        \n",
    "        # Fill the table, a list of lists.\n",
    "        table = [\n",
    "            [set() for _ in range(len(sentence))]\n",
    "            for _ in range(len(sentence))\n",
    "        ]\n",
    "                \n",
    "        # Iterate word by word. \n",
    "        # Note: The indices may be off by one compared to the \n",
    "        # pseudo code in the book due to pythons lists etc.\n",
    "        for j in range(len(sentence)):\n",
    "            # Put the POS tag for the current terminal in the diagonal cells\n",
    "            print()\n",
    "            table[j][j] = self.grammar.get_lhs_for_rhs([sentence[j]])\n",
    "\n",
    "            for i in range(j, -1, -1):\n",
    "                for k in range(i, j):\n",
    "                    # NT1 describes the span from i to k,\n",
    "                    # NT2 the one from k to j\n",
    "                    # If we find something that goes X -> NT1 NT2,\n",
    "                    # we can conver the span i to j!\n",
    "                    print(f\"j={j} | i={i}\")\n",
    "\n",
    "                    non_terminals_1 = table[i][k]\n",
    "                    non_terminals_2 = table[k + 1][j]\n",
    "                    \n",
    "                    \n",
    "                    for rhs in itertools.product(non_terminals_1, non_terminals_2):\n",
    "                        # Find a left hand side from rules that produce\n",
    "                        # both non-terminals. Add them (set: union) to the table.\n",
    "                        table[i][j] = table[i][j] | self.grammar.get_lhs_for_rhs(rhs)\n",
    "\n",
    "            \n",
    "        print(self.draw_table(table))\n",
    "        return self.grammar.start in table[0][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T09:44:14.946975Z",
     "start_time": "2017-12-08T09:43:53.030Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules = {\n",
    "    Rule(\"SS\", \"S Punct\".split()),\n",
    "    Rule(\"S\", \"NP VP\".split()),\n",
    "    Rule(\"S\", \"Pron VP\".split()),\n",
    "    Rule(\"NP\", \"Det Noun\".split()),\n",
    "    Rule(\"VP\", \"Verb VP1\".split()),\n",
    "    Rule(\"VP1\", \"PP PP\".split()),\n",
    "    Rule(\"PP\", \"Prep NP\".split()),\n",
    "    Rule(\"PP\", \"Prep Pron\".split()),\n",
    "    Rule(\"Noun\", [\"BBC\"]),\n",
    "    Rule(\"Noun\", [\"decade\"]),\n",
    "    Rule(\"Verb\", [\"worked\"]),\n",
    "    Rule(\"Pron\", [\"He\"]),\n",
    "    Rule(\"Det\", [\"the\"]),\n",
    "    Rule(\"Det\", [\"a\"]),\n",
    "    Rule(\"Prep\", [\"for\"]),\n",
    "    Rule(\"Punct\", [\".\"])\n",
    "}\n",
    "\n",
    "g = Grammar(\"SS\", rules)\n",
    "\n",
    "p = CYKParser(g)\n",
    "\n",
    "print(p.recognize(\"He worked for the BBC for a decade .\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
